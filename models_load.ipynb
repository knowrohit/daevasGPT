{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "model = model.eval()\n",
    "\n",
    "tokenizer.save_pretrained(\"/content/drive/MyDrive/bernard/GLM_tokenizer\")\n",
    "model.save_pretrained(\"/content/drive/MyDrive/bernard/GLM_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModelForQuestionAnswering\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
    "import torch\n",
    "\n",
    "sentiment_model_path = \"/content/drive/MyDrive/bernard/distilbert_sentiment.pt\"\n",
    "sentiment_tokenizer_dir = \"/content/drive/MyDrive/bernard/distilbert_sentiment_tokenizer\"\n",
    "sentiment_config_dir = \"/content/drive/MyDrive/bernard/distilbert_sentiment_config\"\n",
    "\n",
    "sentiment_config = DistilBertConfig.from_pretrained(sentiment_config_dir)\n",
    "sentiment_model = DistilBertForSequenceClassification(sentiment_config)\n",
    "sentiment_model.load_state_dict(torch.load(sentiment_model_path))\n",
    "sentiment_tokenizer = DistilBertTokenizer.from_pretrained(sentiment_tokenizer_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(sentiment_model.state_dict(), \"/content/drive/MyDrive/bernard/distilbert_sentiment.pt\")\n",
    "sentiment_tokenizer.save_pretrained(\"/content/drive/MyDrive/bernard/distilbert_sentiment_tokenizer\")\n",
    "sentiment_config = sentiment_model.config\n",
    "sentiment_config.save_pretrained(\"/content/drive/MyDrive/bernard/distilbert_sentiment_config\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "enghitokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "enghimodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-hi\")\n",
    "\n",
    "enghitokenizer.save_pretrained(\"/content/drive/MyDrive/bernard/enghitokenizer\")\n",
    "enghimodel.save_pretrained(\"/content/drive/MyDrive/bernard/enghimodel\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "hiengtokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "\n",
    "hiengmodel = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-hi-en\")\n",
    "hiengtokenizer.save_pretrained(\"/content/drive/MyDrive/bernard/hiengtokenizer\")\n",
    "hiengmodel.save_pretrained(\"/content/drive/MyDrive/bernard/hiengmodel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import os\n",
    "from threading import Event, Thread\n",
    "import requests\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    StoppingCriteria,\n",
    "    StoppingCriteriaList,\n",
    "    TextIteratorStreamer,\n",
    ")\n",
    "\n",
    "\n",
    "model_name = \"mosaic_7b\"\n",
    "max_new_tokens = 1536\n",
    "\n",
    "# # small testing model:\n",
    "# model_name = \"gpt2\"\n",
    "# max_new_tokens = 128\n",
    "\n",
    "auth_token = os.getenv(\"enter you hf token\", None)\n",
    "\n",
    "print(f\"Starting to load the model {model_name} into memory\")\n",
    "\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=auth_token,\n",
    "    max_seq_len=4096,\n",
    ").cuda()\n",
    "tok = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_auth_token=auth_token)\n",
    "\n",
    "stop_token_ids = tok.convert_tokens_to_ids([\"<|im_end|>\", \"<|endoftext|>\"])\n",
    "\n",
    "print(f\"Successfully loaded the model {model_name} into memory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
